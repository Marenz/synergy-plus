\section{Methodology}

Many developers before now have vowed to rewrite Synergy. Unfortunately though,
the intention has never materialized due to lack of motivation, involvement from
the community, and momentum.
To solve this problem, we should plan the new version thoroughly and use the
Scrum Agile methodology which will allow us to deliver features in small 
bursts (which will maintain momentum and community interest).

In version 1, we use Buildbot for the nightly build, so this can be used to run
the integration tests (as explained later in this section) as well as standard 
unit tests (which are also run by developers). See Figure \ref{fig:devTesting}.

\begin{figure}[ht!]
  \centering
  \includegraphics{uml/ad-testing.1}
  \caption{Developer testing cycle}
  \label{fig:devTesting}
\end{figure}

\subsection{Unit Testing}

Open source software should be easy for new developers to modify without too
much risk of breaking the existing functionality. Currently, applying patches 
is very risky, since this usually causes regressions. Unit testing will help 
developers identify regressions before we check code in.

However, some aspects of Synergy are difficult to test, since it always requires
more than one machine to function. We \textit{could} emulate this behavior on a 
single machine by running both the server and client (so that they communicate 
locally), but this only allows us to test the network layer (not the mouse,
keyboard, clipboard, and so on). The solution is to use integration testing
\ldots

\subsection{Integration Testing}

\textbf{TODO:} Many users find problems using DirectX/OpenGL, so we should have
some unit tests that actually run within those 3D platforms. How cool would that
be?! Not sure if the functions we need will be there (e.g. getMousePos(), etc).

\textbf{TODO:} Discuss: Mac licensing and hardware support makes it very 
diffcult to properly test OS X. So to start with (until we can afford more
hardware), we'll only be able to test Mac + other, and not Mac + Mac. Also,
Nick only has 1 Mac, so if it's being used for integration testing (i.e. the
mouse gets moved around every night at 8pm), then it can't really be used 
properly as a dev machine. Actually, Nick needs 3 Mac Minis to have full
integration testing (one of which is for dev). Virtualizing Macs is not
very easy, has bad performance, and can be tricky to maintain.

Actually having to fix regressions is not usually a problem -- the problem is
finding them in the bug in first place, which often falls on the end user. 
Often, the regression will be caused way outside of the developer's capable 
testing scope (e.g. on another platform). The solution is to delegate 
cross-platform testing to an automated process.

Nightly integration testing (in the form of unit tests) will be run on on the 
Buildbot build machines, and will
simulate a fully fledged environment involving many \textit{real} machines. The 
contributor would never run these tests before committing (it wouldn't be 
practical), but it will help detect problems early enough for us to establish
blame on a particular source code change.

In each test environment, there will be 2 Buildbot slaves (1 server, 1 client).
We could have 3 slaves involved, but this may be too complex for now.
To run a test, the Buildbot slaves must make some assumptions; that the client
can communicate with the server via LAN, and that they are all turned on at the 
same time (we will have to do this via a Buildbot schedule). The server and 
client test harnesses depend on each other to be running, which introduces a
risk (i.e. one may have failed to start) -- to solve this, we must synchronize
the two processes.

Figure \ref{fig:integrationTesting} illustrates at which point we synchronize
the client and server test harnesses. If both processes do not meet at the same
sync point in a timely fashion, then we should declare the test a failure (in
this case, maybe one of the Buildbot slaves failed to start). The sync point
``meet'' will occur over the network; the simplest approach is to have one 
master (listening), to which one or more slaves will connect. The processes are
considered synchronized when all clients involved connect to the server, and
the sync will fail if the server does not hear from all clients within a 
specific time window (e.g. 1 minute).

All integration test cases must be implemented \textbf{before} we start 
implementing any functionality (so that it does not get postponed). Because the
integration testing is so important, it is effectively our 
Achilles Heel. We must understand that the success of the project (and future 
maintenance) depends very heavily on the build system.

We must design this test each time for testing the clipboard, keyboard input,
screensaver sync, and all other major features. Each test should run once for 
every combination of platform that we support. For example: Windows/Windows,
Windows/Linux, Mac/Linux, etc.

\begin{figure}[ht!]
  \centering
  \includegraphics{uml/ad-int-test.1}
  \caption{Integration testing for mouse movement}
  \label{fig:integrationTesting}
\end{figure}

\subsection{User Stores}

The problem with defining stories for multi-platform, client-server software
is that most features must be implemented once for each platform, then for both
client and server (even more so when the software is integrated tightly with
each platform). This means we need to implement many stories up to 6 times.

\textit{Note: This is not an exhaustive list of features.}

\textbf{As a user:}

\begin{itemize}
  \item I want to move my server mouse to a client computer (and back) so that 
    I only need one mouse.
  \item I want keys pressed on the server to go to the same computer as the
    mouse cursor so that I only need one keyboard.
  \item I want data copied to my clipboard to replicate to the computer which
    I move my cursor to, so that I can copy data between computers.
  \item I want screen savers on all connected computers to start at the same
    time so that all screens stay on while I'm working.
  \item \ldots
\end{itemize}

\textbf{For each:}

\begin{itemize}
  \item Windows server
  \item Windows client
  \item Linux server
  \item Linux client
  \item Mac OS X server
  \item Mac OS X client
\end{itemize}

With these very simple looking user stories, we should be able to make a good 
start, but even the first release must work (if only with little platform
support).
