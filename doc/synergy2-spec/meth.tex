\section{Methodology}

Many developers before now have vowed to rewrite Synergy. Unfortunately though,
the intention has never materialized due to lack of motivation, involvement from
the community, and momentum.
To solve this problem, we should plan the new version thoroughly and use the
Scrum Agile methodology which will allow us to deliver features in small 
bursts (which will maintain momentum and community interest).

In version 1, we use Buildbot for the nightly build, so this can be used to run
the integration tests (as explained later in this section) as well as standard 
unit tests (which are also run by developers). See Figure \ref{fig:devTesting}.

\begin{figure}[ht!]
  \centering
  \includegraphics{diag/ad-testing.1}
  \caption{Developer testing cycle}
  \label{fig:devTesting}
\end{figure}

\subsection{Unit Testing}

Open source software should be easy for new developers to modify without too
much risk of breaking the existing functionality. Currently, applying patches 
is very risky, since this usually causes regressions. Unit testing will help 
developers identify regressions before we check code in. All planned unit tests
will be written \textbf{before} the actual software.

Many users find problems using computer games. These are usually problems with
the mouse - e.g. it might skip, stall, or do crazy things. So to solve this 
problem, we will have a set of unit tests that actually run within DirectX and
OpenGL (depending on platform support). And since these will be written before
the actual code, it will force us to either solve the problem early, or 
officially declare DirectX and/or OpenGL as unsupported.

However, some aspects of Synergy are difficult to test, since it always requires
more than one machine to function. We \textit{could} emulate this behavior on a 
single machine by running both the server and client (so that they communicate 
locally), but this only allows us to test the network layer (not the mouse,
keyboard, clipboard, and so on). The solution is to use integration testing
\ldots

\subsection{Integration Testing}

\begin{figure}[ht!]
  \centering
  \includegraphics{diag/ad-int-test.1}
  \caption{Integration testing for mouse movement}
  \label{fig:integrationTesting}
\end{figure}

Actually having to fix regressions is not usually a problem -- the problem is
finding them in the bug in first place, which often falls on the end user. 
Often, the regression will be caused way outside of the developer's capable 
testing scope (e.g. on another platform). The solution is to delegate 
cross-platform testing to an automated process.

Nightly integration testing (in the form of unit tests) will be run on on the 
Buildbot build machines, and will simulate a fully fledged environment involving
many \textit{real} machines. The contributor would never run these tests before
committing (it wouldn't be  practical), but it will help detect problems early
enough for us to establish blame on a particular source code change.

In each test environment, there will be 2 Buildbot slaves (1 server, 1 client).
We could have 3 slaves involved, but this may be too complex for now.
To run a test, the Buildbot slaves must make some assumptions; that the client
can communicate with the server via LAN, and that they are all turned on at the 
same time (we will have to do this via a Buildbot schedule). The server and 
client test harnesses depend on each other to be running, which introduces a
risk (i.e. one may have failed to start) -- to solve this, we must synchronize
the two processes. We must design this test each time for testing the clipboard,
keyboard input, screensaver sync, and all other major features. Each test should
run once for every combination of platform that we support, except for 
homogeneous platform configurations (e.g. Windows on Windows) since this is
pointless (the communication protocol is common).

Figure \ref{fig:integrationTesting} illustrates at which point we synchronize
the client and server test harnesses. If both processes do not meet at the same
sync point in a timely fashion, then we should declare the test a failure (in
this case, maybe one of the Buildbot slaves failed to start). The sync point
``meet'' will occur over the network; the simplest approach is to have one 
master (listening), to which one or more slaves will connect. The processes are
considered synchronized when all clients involved connect to the server, and
the sync will fail if the server does not hear from all clients within a 
specific time window (e.g. 1 minute).

Like normal unit tests, all integration test cases must be implemented 
\textbf{before} we start implementing any functionality (so that the integration
testing does not get postponed). Because the integration testing is so 
important, it is effectively a weakness - if it goes away, then the project will
start to become unstable; the success of the project (and future maintenance) 
depends very heavily on the build system. Another limitation is that because
all integration test communication must be done over LAN, all machines involved
must be hosted on a single build farm. In future, we should consider having two
build farms, hosted in geographically dispersed locations (given that the build
farm is key to our success).

Apple licensing and hardware support makes it very difficult to test software on
Mac OS X automatically. Before we can start work on this version, the build farm
must have a dedicated Apple Mac computer. This is because it will boot in round
robin fashion (\textbf{TODO:} prove this is actually possible!) between all 
supported versions of Mac OS X. Realistically, you couldn't use this machine 
for active development. The version 1 build farm currently has a Mac Mini, but
this is used for part-time development, making the round-robin booting
impractical. We could virtualize the Mac OS X machines using VMware, but often
Mac OS X does not play nice with virtualization. However, virtualization this 
might be something to consider if we can use VMware Server (the build machine
will be headless).

\subsection{User Stories}

The problem with defining stories for multi-platform, client-server software
is that most features must be implemented once for each platform, then for both
client and server (even more so when the software is integrated tightly with
each platform). This means we need to implement many stories up to 6 times.

\textit{Note: This is not an exhaustive list of features.}

\textbf{As a user:}

\begin{itemize}
  \item I want to move my server mouse to a client computer (and back) so that 
    I only need one mouse.
  \item I want keys pressed on the server to go to the same computer as the
    mouse cursor so that I only need one keyboard.
  \item I want data copied to my clipboard to replicate to the computer which
    I move my cursor to, so that I can copy data between computers.
  \item I want screen savers on all connected computers to start at the same
    time so that all screens stay on while I'm working.
  \item \ldots
\end{itemize}

\textbf{For each:}

\begin{itemize}
  \item Windows server
  \item Windows client
  \item Linux server
  \item Linux client
  \item Mac OS X server
  \item Mac OS X client
\end{itemize}

With these very simple looking user stories, we should be able to make a good 
start, but even the first release must work (if only with little platform
support).
